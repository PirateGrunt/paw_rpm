{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Scikit-Learn to identify the line of business of a loss triangle\n",
    "We would like to know whether we can use Machine Learning to predict the line of business of a triangle from just the triangle itself.  We will download the *CAS loss reserve database* for Medical Malpractice, Private Passenger Auto, and Workers' Compensation. \n",
    "\n",
    "In this notebook we will explore some of the tooling available in scikit-learn to do this, and as we learn about `sklearn` we will simultanously answer this question.\n",
    "\n",
    "But first, let's use `pandas` to grab the data of the 19 largest carriers for each line of business, and group the remaining carriers as 'Other'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "ldf = pd.read_csv(\n",
    "    r'https://raw.githubusercontent.com/PirateGrunt/paw_rpm/master/notebooks/assets/links.csv',\n",
    "    index_col=['GRNAME','LOB'])\n",
    "ldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Scikit-Learn?\n",
    "\n",
    "**Machine Learning in Python**\n",
    "\n",
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### scikit-learn covers the majority of supervised and unsupervised ML techniques available today and  is continually expanding\n",
    "![](https://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`sklearn` is the defacto standard Machine Learning API for Python.  Other libraries yield to the simplicity of its API. \n",
    "\n",
    "![](https://github.com/PirateGrunt/paw_rpm/blob/master/notebooks/assets/one_api.png?raw=true)\n",
    "\n",
    "* Want to do some Keras Deep learning?  No problem, just use `keras.wrappers.scikit_learn`\n",
    "* XGBoost anyone?  Use: `xgboost.sklearn`\n",
    "* Don't want to learn the syntax for the Light GBM? `lightgbm.sklearn` to the rescue.\n",
    "* Natural langauge processing requires unique functionality, right? Nope, `nltk.classify.scikitlearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Scikit-learn is a consistent API for all Machine Learning Algorithms\n",
    "\n",
    "Estimators are the building block of scikit-learn.  Almost everything is an estimator.  All estimators have `fit()` methods. Most have either a `predict()` or `transform()` method. Supervised techniques generally have a `score()` method as well.\n",
    "\n",
    "The basic ML workflow looks like this:\n",
    "```python\n",
    "from sklearn.EstimatorFamily import Estimator\n",
    "est = Estimator(hyperparameter_1, ... ,hyperparameter_n) # Create a model\n",
    "est.fit(X_train, y_train) # Fit the model\n",
    "est.score(X_test, y_test) # Evaluate model efficacy\n",
    "est.predict(X_test) # Create predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Importing your estimators\n",
    "`from sklearn.EstimatorFamily import Estimator` is typically how you'd import an estimator.  Some examples are:\n",
    "``` python\n",
    "from sklearn.linear_model import RidgeRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Import the support vector classifier and a k-neighbors classifier\n",
    "# from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Hyperparameters of your estimarors (Controlling how your estimator fits)\n",
    "Instantiating an estimator typically looks like `est = Estimator(hyperparameter_1, ... ,hyperparameter_n)`.\n",
    "Upon instantition you have the *option* of setting hyperparameters (i.e. parameters whose values are set before the learning process).  All hyperparameters have defaults that may or may not be satisfactory for your particular problem.\n",
    "\n",
    "Exmaples of setting initial hyperparameters on an estimator:\n",
    "```python\n",
    "rr = RidgeRegression(alpha=0.5, fit_intercept=False, normalize=True)\n",
    "knc = KNeighborsClassifier(n_neighbors=10)\n",
    "gbc = GradientBoostingClassifier()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Override the SVC hyperparameters such that it uses a kernel type\n",
    "# of a second degree polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transformers - a special kind of estimator\n",
    "Several `sklearn` estimators implement a `transform()` method.  Transformers are typically used to 'transform' your featureset in a way that will improve another algorithms (e.g. regressor, classifier) performance.\n",
    "\n",
    "Typical examples include:\n",
    "```python\n",
    "sklearn.preprocessing.PCA # Principle Components transformation\n",
    "sklearn.preprocessing.OneHotEncoder # Categorical to dummy transformation\n",
    "sklearn.preprocessing.StandardScaler # Removing the mean and scaling to unit variance for each feature\n",
    "sklearn.preprocessing.LabelEncoder # Single-column label to integer tranformation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Import and create a labelEncoder transformer named 'le'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Fitting an estimator\n",
    "To fit an estimator we require data - in most cases the data must be `numpy` arrays that are numeric in nature. However, many of the preprocessing transformers are helpers designed to meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "response = ldf.reset_index()['LOB']\n",
    "# Exercise - Pass 'response'to the fit method of your LabelEncoder() instance\n",
    "# you created in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Mutating the Estimator with fit()\n",
    "Though it looks like nothing happened, a lot happened under the hood.  Our estimator has seen data can now be applied to new datasets.  Once an estimator is fit, it spin off useful metadata that describes the fit model.  `sklearn` uses a trailing underscore in property names to help users distinguish between hyperparameters and the new metadata.\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(X, y)\n",
    "print(lr.fit_intercept) # A hyperparameter.  Returns False.\n",
    "print(lr.coef_) # Trailing underscore denotes the property comes from a 'fit'.  Returns model coefficients.\n",
    "```\n",
    "\n",
    "Additionally the predict, transform, and score methods (if applicable) become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - access the 'classes_' property of our label encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transforming  a simple dataset\n",
    "With a fit estimator we can create predictions or transformations on any new dataset that has the same number of features as our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise -  create an array called 'y' that uses your LabelEncoder\n",
    "# transform() method on 'response'.  Display y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Learning Example - Identifying the line of business of an unlabeled triangle\n",
    "We've computed the volume weighted development patterns of twenty companies for each line of business, `wkcomp`, `comauto`, and `ppauto` and want to use them to train a Machine Learning model that can identify the appropriate line of business.\n",
    "\n",
    "Defining this problem more concretely:<br>\n",
    "The LDFs are our featureset, **X**, and the known line of business is our response, **y**.\n",
    "\n",
    "`sklearn` generally likes to consume `numpy` arrays.  It does not like mixed datatypes like a `pandas.DataFrame`.  The supervised learning estimators particularly like `numpy` arrays to be strictly numeric.  We've already created a `numpy` array for **y** using `LabelEncoder`.\n",
    "\n",
    "Fortunately, our LDFs are already numeric, but we just need to convert them to a `numpy` array.  This is done esiest using the `values` attribute of our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - create a matrix called X that is equal to ldf.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Train/Test Split\n",
    "It is best practice in machine learning to evaluate models on a test set of data.  Since this is covered substantially in other literature, we will not go into the details of why here.  `sklearn` comes with several utilities to split data, but we will explore the simplest one.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "```\n",
    "\n",
    "`train_test_split` returns a tuple of our features/response split into training and test sets. The `random_state` argument shows up in a lot of places in `sklearn`.  Generally, when there is a stochastic component to the `sklearn` component you are using, `random_state` is there to allow you to set a seed so that your work can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - using the sample code in the previous cell, split the loss\n",
    "# reserving data into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Fitting our classifier\n",
    "Our data is in a numerical format, its been split, and now we are ready to do some Machine Learning.  \n",
    "\n",
    "Don't forget, when fitting any supervised learning technique, you must specify both your featureset and your response in the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Create a KNeighborsClassifier, knn and fit it to our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Evaluating our classifier\n",
    "The `score()` method of all classifiers defaults to an accuracy measure.  For regressors, it will return an R-squared figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Use score() to evaluate the accuracy of our KNeighborsClassifier\n",
    "# on our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Classifier Confusion Matrix\n",
    "Another way of looking at a classifier's performance is by way of its `confusion_matrix` which gives a bit more information than our accuracy score.  Specifically, it tells us our false positive and false negative rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test, knn.predict(X_test)),\n",
    "             index=le.classes_, columns=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try another classifier\n",
    "Remember the `sklearn` API was designed to make using different algorithms as consistent as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - Fit a LogisticRegression and evaluate its accuracy on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visual representation of first three Development Factors\n",
    "By inspection (at least across the first three development ages), it is more difficult to distinguish between `wkcomp` and `ppauto` in line with where our classifiers are least accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "g = sns.pairplot(ldf.reset_index()[['LOB','1-2','2-3','3-4']], hue=\"LOB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Robustness of train_test_split\n",
    "Let's test the performance of our `KNeighborsClassifier` using a different random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=41)\n",
    "# Exercise - What is our accuracy when we change our train_test_split\n",
    "# random_state to 41?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Cross-validation\n",
    "* `sklearn` provides a `cross_val_score` to test the accuracy of an estimator across multiple folds painting a truer picture of an estimators' efficacy than a simple train/test split.\n",
    "* With `cross_val_score`, we don't really need to provide separate train and test sets.  Though, with enough data, it is sometimes instructive to have train/test and holdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "np.mean(cross_val_score(knn, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Improving model accuracy with GridSearchCV\n",
    "\n",
    "With `GridSearchCV`, we can feed a hyperparameter grid into our estimator to determine an 'optimal' set of hyperparameters to use for our particular business problem.  `GridSearchCV` itself is an estimator and so it has the usual `'fit()` and `predict()` methods any other classifier would.\n",
    "\n",
    "At a minimum, parameterizing the GridSearchCV estimator we need to specify:\n",
    "1. The estimator we want to use\n",
    "2. The hyperparameter searchspace as a dictionary\n",
    "\n",
    "Optionally, we can also specify:\n",
    "1. The number of folds to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={'n_neighbors':[1,3,5,7,9,11]}\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, refit=True)\n",
    "grid.fit(X, y)\n",
    "print(f'Best Accuracy Score: {grid.best_score_}')\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Visual inspection of the cross-validated scores shows support for `n_neighbors=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pointplot(x=grid.cv_results_['param_n_neighbors'],\n",
    "                  y=grid.cv_results_['mean_test_score']) \\\n",
    "       .set(xlabel='n_neighbors', ylabel='Accuracy', title='Gridsearch Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Explore values of KNeighborsClassifier hyperparameter, p to\n",
    "# improve the cross-validated score of our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Display the confusion matrix of your best estimator from the\n",
    "# previous exercise on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More complex workflows with Pipeline\n",
    "\n",
    "The authors of `sklearn` recognize that composability of multiple estimators will be necessary to build the best models.  For example, you may want to cluster a feature before feeding it into a Regressor.\n",
    "\n",
    "The `Pipeline` is useful for chaining one or more transformers together.  Pipelines themselves are estimators and have `fit()`, `predict()`, and `score()` function and can be used with all of the `sklearn` funcitons used for regular estimators including but not limited to: `cross_val_score`, `confusion_martix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Regression Dataset\n",
    "boston = load_boston()\n",
    "    \n",
    "# Polynomial Transformer\n",
    "poly = PolynomialFeatures(degree=2) \n",
    "\n",
    "# Regressor\n",
    "rf = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "# Set up Steps\n",
    "steps=[('poly', poly), ('rf', rf)]\n",
    "\n",
    "# Build a Pipeline\n",
    "pipe = Pipeline(steps=steps) \n",
    "\n",
    "# Cross-Validation R-square\n",
    "np.mean(cross_val_score(pipe, boston['data'], boston['target'], cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Create a Pipeline with Principle Components Analysis (PCA) as a\n",
    "# first step and the optimal KNeighbors hyperparameters (n_neighbors=3 and p=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pipelines and GridSearchCV\n",
    "Since a `Pipeline` is just another estimator `GridSearchCV` allows the hyperparameter space of all estimators in the pipeline to be gridsearched in one go.  \n",
    "\n",
    "To avoid hyperparameter name clashes between one estimator and another within a pipeline, `sklearn` uses a double underscore naming convention of the form {estimator_name}__{hyperparameter} for the keys of its parameter grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = dict(rf__n_estimators=[10, 25],\n",
    "                  rf__max_depth=[10, 15, 20],\n",
    "                  rf__min_samples_split=[5, 10, 15],\n",
    "                  poly__degree=[1, 2])\n",
    "\n",
    "# Set up Steps\n",
    "steps=[('poly', PolynomialFeatures()), ('rf', RandomForestRegressor())]\n",
    "\n",
    "# Build a Pipeline\n",
    "pipe = Pipeline(steps=steps) \n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = dict(rf__n_estimators=[10, 25],\n",
    "                  rf__max_depth=[10, 15, 20],\n",
    "                  rf__min_samples_split=[5, 10, 15],\n",
    "                  poly__degree=[1, 2])\n",
    "\n",
    "#Grid Search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, iid=True)\n",
    "grid.fit(boston['data'], boston['target'])\n",
    "\n",
    "print(f'Best R-square: {grid.best_score_}')\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Use Pipeline and GridsearchCV to determine whether we achieve a\n",
    "# better cross-validated accuracy using PCA and KNeighborsClassifier combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scikit-Learn Recap\n",
    "\n",
    "* Almost everything is an Estimator.  They all have a `fit` method and depending on the nature of the estimator may also have a `predict`, `score` or `transform` method.\n",
    "* The API is standardized across estimator\n",
    "* A transformer is a special type of estimator that trasnforms data for another Estimator\n",
    "* Cross-validation with Grid Search helps in hyperparameter selection\n",
    "* Pipelines are useful for composing a chain of Estimators.\n",
    "* The documentation is a goldmine of information"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
