{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Scikit-Learn to identify the line of business of a loss triangle\n",
    "We would like to know whether we can use Machine Learning to predict the line of business of a triangle from just the triangle itself.  We will download the *CAS loss reserve database* for Medical Malpractice, Private Passenger Auto, and Workers' Compensation. \n",
    "\n",
    "In this notebook we will explore some of the tooling available in scikit-learn to do this, and as we learn about `sklearn` we will simultanously answer this question.\n",
    "\n",
    "But first, let's use `pandas` to grab the data of the 19 largest carriers for each line of business, and group the remaining carriers as 'Other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "data_url = 'https://www.casact.org/research/reserve_data'\n",
    "# Read in the data\n",
    "lobs = ['medmal','ppauto','wkcomp']\n",
    "data = pd.DataFrame()\n",
    "columns = ['GRCODE','GRNAME','AccidentYear','DevelopmentYear','DevelopmentLag'\n",
    "           ,'IncurLoss', 'CumPaidLoss','BulkLoss','EarnedPremDIR'\n",
    "           ,'EarnedPremCeded','EarnedPremNet', 'Single','PostedReserve97']\n",
    "for lob in lobs:\n",
    "    file_url = f'{data_url}/{lob}_pos.csv'\n",
    "    subset = pd.read_csv(file_url, names=columns, skiprows=1)\n",
    "    subset['LOB'] = lob\n",
    "    data = data.append(subset, sort=True)\n",
    "data = data[data['DevelopmentYear']<=1997].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "def make_trg(data):\n",
    "    # Find largest 20 companies by premium size for each LOB\n",
    "    aggregates = (data[data['DevelopmentYear']==1997].groupby(['LOB','GRNAME']) \\\n",
    "                                                 .sum()['IncurLoss']) \\\n",
    "                                                 .reset_index()\n",
    "    top_20_by_lob = aggregates.iloc[aggregates.groupby('LOB')['IncurLoss'] \\\n",
    "                              .nlargest(19).index.levels[1]]\n",
    "    data2 = data.merge(top_20_by_lob, how='left', on=['LOB','GRNAME'])\n",
    "    data2.loc[data2.iloc[:,-1].isna(),'GRNAME'] = 'Other'\n",
    "    \n",
    "    # Create Triangles\n",
    "    triangles = pd.pivot_table(data2, index=['GRNAME','LOB','AccidentYear'],\n",
    "                               columns='DevelopmentLag', values='CumPaidLoss',\n",
    "                               aggfunc='sum')\n",
    "    \n",
    "    # Determine LDF Weights\n",
    "    weight = np.array(~triangles.iloc[:,1:].isna())\n",
    "    columns = [f'{triangles.columns[num]}-{triangles.columns[num+1]}'\n",
    "               for num, item in enumerate(triangles.columns[:-1])]\n",
    "\n",
    "    # Volume-weighted numerator and demoninator\n",
    "    numerator = (\n",
    "        (triangles.iloc[:,1:]).reset_index() \n",
    "                                     .drop('AccidentYear',axis=1)\n",
    "                                     .groupby(['GRNAME','LOB'])\n",
    "                                     .sum(axis=0))\n",
    "    denominator = (\n",
    "        (weight*triangles.iloc[:,:-1]).reset_index()\n",
    "                                      .drop('AccidentYear',axis=1)\n",
    "                                      .groupby(['GRNAME','LOB'])\n",
    "                                      .sum(axis=0))\n",
    "    numerator.columns = denominator.columns = columns\n",
    "\n",
    "    # Development Patterns\n",
    "    ldf = (numerator/denominator).fillna(1.0)\n",
    "    \n",
    "    return ldf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ldf3 = make_trg(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatives, including original\n",
    "def make_trg_2(data):\n",
    "    '''\n",
    "    see _alt that this method is fastest\n",
    "    '''\n",
    "    aggregates2 = data.query(' DevelopmentYear ==  1997 ').groupby(['LOB','GRNAME'])['IncurLoss'].sum() \n",
    "    top_20_by_lob = aggregates2.groupby(level='LOB').apply(lambda x : x.nlargest(19).reset_index(level=0, drop=True))\n",
    "    \n",
    "    data_alt2 = data.merge(top_20_by_lob.to_frame(), how='left', left_on=['LOB','GRNAME'], right_index=True)\n",
    "    data_alt2.loc[data_alt2.loc[:,'IncurLoss_y'].isna(), 'GRNAME'] = 'Other'\n",
    "    \n",
    "    # create triangles \n",
    "    triangles = pd.pivot_table(data_alt2, index=['GRNAME','LOB','AccidentYear'],\n",
    "                           columns='DevelopmentLag', values='CumPaidLoss')\n",
    "    \n",
    "    # Determine LDF Weights ORIG\n",
    "    w = pd.DataFrame(np.array([[1 if i+j<9 else 0 for i in range(9)] for j in range(10)]))\n",
    "    weight = np.tile(w, (int(triangles.shape[0]/10), 1))\n",
    "    columns = [f'{triangles.columns[num]}-{triangles.columns[num+1]}'\n",
    "               for num, item in enumerate(triangles.columns[:-1])]\n",
    "\n",
    "    # Volume-weighted numerator and demoninator mask for denom only; values on num because want index from num \n",
    "    ldf = (triangles.iloc[:,1:].groupby(level=['GRNAME','LOB']).sum().values / \\\n",
    "           (weight*triangles.iloc[:,:-1]).groupby(level=['GRNAME','LOB']).sum()).fillna(1.0) \n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = make_trg_2(data)\n",
    "np.allclose(a0, aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.6 ms ± 708 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit aa = make_trg_2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatives, including original\n",
    "def make_trg_alt(data, method):\n",
    "        \n",
    "    aggregates2 = data.query(' DevelopmentYear ==  1997 ').groupby(['LOB','GRNAME'])['IncurLoss'].sum() \n",
    "    top_20_by_lob = aggregates2.groupby(level='LOB').apply(lambda x : x.nlargest(19).reset_index(level=0, drop=True))\n",
    "    \n",
    "    data_alt2 = data.merge(top_20_by_lob.to_frame(), how='left', left_on=['LOB','GRNAME'], right_index=True)\n",
    "    data_alt2.loc[data_alt2.loc[:,'IncurLoss_y'].isna(), 'GRNAME'] = 'Other'\n",
    "    \n",
    "    # create triangles \n",
    "    triangles = pd.pivot_table(data_alt2, index=['GRNAME','LOB','AccidentYear'],\n",
    "                           columns='DevelopmentLag', values='CumPaidLoss')\n",
    "    \n",
    "    # Determine LDF Weights ORIG\n",
    "#     weight = np.array(~triangles.iloc[:,1:].isna())\n",
    "    w = pd.DataFrame(np.array([[1 if i+j<9 else 0 for i in range(9)] for j in range(10)]))\n",
    "    weight = np.tile(w, (int(triangles.shape[0]/10), 1))\n",
    "    columns = [f'{triangles.columns[num]}-{triangles.columns[num+1]}'\n",
    "               for num, item in enumerate(triangles.columns[:-1])]\n",
    "\n",
    "    # Volume-weighted numerator and demoninator; do not need mask here... \n",
    "    if method=='original':\n",
    "        numerator = (\n",
    "            (triangles.iloc[:,1:]).reset_index() \n",
    "                                         .drop('AccidentYear',axis=1)\n",
    "                                         .groupby(['GRNAME','LOB'])\n",
    "                                         .sum(axis=0))\n",
    "        denominator = (\n",
    "            (weight*triangles.iloc[:,:-1]).reset_index()\n",
    "                                          .drop('AccidentYear',axis=1)\n",
    "                                          .groupby(['GRNAME','LOB'])\n",
    "                                          .sum(axis=0))\n",
    "    if method=='original_noax':\n",
    "        numerator = (\n",
    "            (triangles.iloc[:,1:]).reset_index() \n",
    "                                         .drop('AccidentYear',axis=1)\n",
    "                                         .groupby(['GRNAME','LOB'])\n",
    "                                         .sum())\n",
    "        denominator = (\n",
    "            (weight*triangles.iloc[:,:-1]).reset_index()\n",
    "                                          .drop('AccidentYear',axis=1)\n",
    "                                          .groupby(['GRNAME','LOB'])\n",
    "                                          .sum())\n",
    "    if method=='original2':\n",
    "        numerator = (\n",
    "            (triangles.iloc[:,1:]).reset_index() \n",
    "                                         .drop('AccidentYear',axis=1)\n",
    "                                         .groupby(['GRNAME','LOB'])\n",
    "                                         .apply(lambda x: np.sum(x.iloc[:, 2:], axis=0)))\n",
    "        denominator = (\n",
    "            (weight*triangles.iloc[:,:-1]).reset_index()\n",
    "                                          .drop('AccidentYear',axis=1)\n",
    "                                          .groupby(['GRNAME','LOB'])\n",
    "                                          .apply(lambda x: np.sum(x.iloc[:, 2:], axis=0)))\n",
    "    if method=='alt1':\n",
    "        numerator = (weight*triangles.iloc[:,1:]).groupby(level=['GRNAME','LOB']).sum()\n",
    "        denominator = (weight*triangles.iloc[:,:-1]).groupby(level=['GRNAME','LOB']).sum()\n",
    "    if method=='alt2':    \n",
    "        numerator = triangles.iloc[:,1:].groupby(level=['GRNAME','LOB']).apply(sum)\n",
    "        denominator = (weight*triangles.iloc[:,:-1]).groupby(level=['GRNAME','LOB']).apply(sum)\n",
    "\n",
    "#     print(numerator.shape, denominator.shape, len(columns))\n",
    "    numerator.columns = denominator.columns = columns\n",
    "\n",
    "    # Development Patterns\n",
    "    ldf = (numerator/denominator).fillna(1.0) \n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:4317: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  stacked_values = np.vstack(map(np.asarray, values))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = make_trg(data)\n",
    "a1 = make_trg_alt(data, 'original')\n",
    "a1x = make_trg_alt(data, 'original_noax')\n",
    "a1s = make_trg_alt(data, 'original2')\n",
    "a2 = make_trg_alt(data, 'alt1')\n",
    "a3 = make_trg_alt(data, 'alt2')\n",
    "np.allclose(a0, a1), np.allclose(a0, a1x),  np.allclose(a0, a2), np.allclose(a0, a3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 ms ± 26.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a0 = make_trg(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 ms ± 14.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a1 = make_trg_alt(data, 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.1 ms ± 9.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a1 = make_trg_alt(data, 'original_noax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.3 ms ± 569 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a1 = make_trg_alt(data, 'alt1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.5 ms ± 5.77 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a1 = make_trg_alt(data, 'alt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.4 ms ± 23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a1 = make_trg_alt(data, 'original2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrived example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(np.random.normal(size=(20000,2000)))\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.groupby(lambda x: x // 5000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:1].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:1].groupby(lambda x: x // 500).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:4].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:9].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:19].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:1].groupby(lambda x: x // 20000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:1].groupby(lambda x: x // 500).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:4].groupby(lambda x: x // 20000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:9].groupby(lambda x: x // 20000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:19].groupby(lambda x: x // 20000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:5].groupby(lambda x: x // 20000).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.loc[:, 0:5].groupby(lambda x: x // 20000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.groupby(lambda x: x // 5000).apply(sum, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tdf.groupby(lambda x: x // 5000).apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = tdf.groupby(lambda x: x // 5000).sum(axis=0)\n",
    "a2 = tdf.groupby(lambda x: x // 5000).apply(sum, axis=0)\n",
    "np.allclose(a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t1(tdf):\n",
    "    tdf.groupby(lambda x: x // 5000).apply(sum, axis=0)\n",
    "\n",
    "def t2(tdf):\n",
    "    tdf.groupby(lambda x: x // 5000).apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.dis(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.dis(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfun(a):\n",
    "    return a\n",
    "\n",
    "def myfunex(a, **kwargs):\n",
    "    return a\n",
    "\n",
    "def tester1(a):\n",
    "    t = 0\n",
    "    for i in range(a):\n",
    "        t += myfun(i)\n",
    "\n",
    "def tester2(a):\n",
    "    t = 0\n",
    "    for i in range(a):\n",
    "        t += myfunex(i, other=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.dis(tester1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.dis(tester2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tester1(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tester2(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit myfun(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit myfunex(12, extra=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.kwlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(str_in, what='aeiou '):\n",
    "    return ''.join([i if i in what else '_' for i in str_in.lower() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count('Stephen Mildenhall', 'dhl ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating Loss Development Factors to be used as features in our Machine Learning Model(s).\n",
    "\n",
    "Let's write a function that allows us to generate volume-weighted development patterns for each of company/LOB above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Determine LDF Weights\n",
    "weight = np.array(~triangles.iloc[:,1:].isna())\n",
    "columns = [f'{triangles.columns[num]}-{triangles.columns[num+1]}'\n",
    "           for num, item in enumerate(triangles.columns[:-1])]\n",
    "\n",
    "# Volume-weighted numerator and demoninator\n",
    "numerator = (\n",
    "    (weight*triangles.iloc[:,1:]).reset_index() \n",
    "                                 .drop('AccidentYear',axis=1)\n",
    "                                 .groupby(['GRNAME','LOB'])\n",
    "                                 .sum(axis=0))\n",
    "denominator = (\n",
    "    (weight*triangles.iloc[:,:-1]).reset_index()\n",
    "                                  .drop('AccidentYear',axis=1)\n",
    "                                  .groupby(['GRNAME','LOB'])\n",
    "                                  .sum(axis=0))\n",
    "numerator.columns = denominator.columns = columns\n",
    "\n",
    "# Development Patterns\n",
    "ldf = (numerator/denominator).fillna(1.0)\n",
    "ldf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Scikit-Learn?\n",
    "\n",
    "**Machine Learning in Python**\n",
    "\n",
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### scikit-learn covers the majority of supervised and unsupervised ML techniques available today and  is continually expanding\n",
    "![](https://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`sklearn` is the defacto standard Machine Learning API for Python.  Other libraries yield to the simplicity of its API. \n",
    "\n",
    "![](https://github.com/PirateGrunt/paw_rpm/blob/master/notebooks/assets/one_api.png?raw=true)\n",
    "\n",
    "* Want to do some Keras Deep learning?  No problem, just use `keras.wrappers.scikit_learn`\n",
    "* XGBoost anyone?  Use: `xgboost.sklearn`\n",
    "* Don't want to learn the syntax for the Light GBM? `lightgbm.sklearn` to the rescue.\n",
    "* Natural langauge processing requires unique functionality, right? Nope, `nltk.classify.scikitlearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Scikit-learn is a consistent API for all Machine Learning Algorithms\n",
    "\n",
    "Estimators are the building block of scikit-learn.  Almost everything is an estimator.  All estimators have `fit()` methods. Most have either a `predict()` or `transform()` method. Supervised techniques generally have a `score()` method as well.\n",
    "\n",
    "The basic ML workflow looks like this:\n",
    "```python\n",
    "from sklearn.EstimatorFamily import Estimator\n",
    "est = Estimator(hyperparameter_1, ... ,hyperparameter_n) # Create a model\n",
    "est.fit(X_train, y_train) # Fit the model\n",
    "est.score(X_test, y_test) # Evaluate model efficacy\n",
    "est.predict(X_test) # Create predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Importing your estimators\n",
    "`from sklearn.EstimatorFamily import Estimator` is typically how you'd import an estimator.  Some examples are:\n",
    "``` python\n",
    "from sklearn.linear_model import RidgeRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Import the support vector classifier and a k-neighbors classifier\n",
    "# from sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Hyperparameters of your estimarors (Controlling how your estimator fits)\n",
    "Instantiating an estimator typically looks like `est = Estimator(hyperparameter_1, ... ,hyperparameter_n)`.\n",
    "Upon instantition you have the *option* of setting hyperparameters (i.e. parameters whose values are set before the learning process).  All hyperparameters have defaults that may or may not be satisfactory for your particular problem.\n",
    "\n",
    "Exmaples of setting initial hyperparameters on an estimator:\n",
    "```python\n",
    "rr = RidgeRegression(alpha=0.5, fit_intercept=False, normalize=True)\n",
    "knc = KNeighborsClassifier(n_neighbors=10)\n",
    "gbc = GradientBoostingClassifier()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Override the SVC hyperparameters such that it uses a kernel type\n",
    "# of a second degree polynomial\n",
    "SVC(kernel='poly', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transformers - a special kind of estimator\n",
    "Several `sklearn` estimators implement a `transform()` method.  Transformers are typically used to 'transform' your featureset in a way that will improve another algorithms (e.g. regressor, classifier) performance.\n",
    "\n",
    "Typical examples include:\n",
    "```python\n",
    "sklearn.preprocessing.PCA # Principle Components transformation\n",
    "sklearn.preprocessing.OneHotEncoder # Categorical to dummy transformation\n",
    "sklearn.preprocessing.StandardScaler # Removing the mean and scaling to unit variance for each feature\n",
    "sklearn.preprocessing.LabelEncoder # Single-column label to integer tranformation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Import and create a labelEncoder transformer named 'le'\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Fitting an estimator\n",
    "To fit an estimator we require data - in most cases the data must be `numpy` arrays that are numeric in nature. However, many of the preprocessing transformers are helpers designed to meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "response = ldf.reset_index()['LOB']\n",
    "# Exercise - Pass 'response'to the fit method of your LabelEncoder() instance\n",
    "# you created in the previous exercise.\n",
    "le.fit(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Mutating the Estimator with fit()\n",
    "Though it looks like nothing happened, a lot happened under the hood.  Our estimator has seen data can now be applied to new datasets.  Once an estimator is fit, it spin off useful metadata that describes the fit model.  `sklearn` uses a trailing underscore in property names to help users distinguish between hyperparameters and the new metadata.\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(X, y)\n",
    "print(lr.fit_intercept) # A hyperparameter.  Returns False.\n",
    "print(lr.coef_) # Trailing underscore denotes the property comes from a 'fit'.  Returns model coefficients.\n",
    "```\n",
    "\n",
    "Additionally the predict, transform, and score methods (if applicable) become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - access the 'classes_' property of our label encoder.\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transforming  a simple dataset\n",
    "With a fit estimator we can create predictions or transformations on any new dataset that has the same number of features as our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise -  create an array called 'y' that uses your LabelEncoder\n",
    "# transform() method on 'response'.  Display y.\n",
    "y = le.transform(response)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Learning Example - Identifying the line of business of an unlabeled triangle\n",
    "We've computed the volume weighted development patterns of twenty companies for each line of business, `wkcomp`, `comauto`, and `ppauto` and want to use them to train a Machine Learning model that can identify the appropriate line of business.\n",
    "\n",
    "Defining this problem more concretely:<br>\n",
    "The LDFs are our featureset, **X**, and the known line of business is our response, **y**.\n",
    "\n",
    "`sklearn` generally likes to consume `numpy` arrays.  It does not like mixed datatypes like a `pandas.DataFrame`.  The supervised learning estimators particularly like `numpy` arrays to be strictly numeric.  We've already created a `numpy` array for **y** using `LabelEncoder`.\n",
    "\n",
    "Fortunately, our LDFs are already numeric, but we just need to convert them to a `numpy` array.  This is done esiest using the `values` attribute of our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - create a matrix called X that isequal to ldf.values\n",
    "X = ldf.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Train/Test Split\n",
    "It is best practice in machine learning to evaluate models on a test set of data.  Since this is covered substantially in other literature, we will not go into the details of why here.  `sklearn` comes with several utilities to split data, but we will explore the simplest one.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "```\n",
    "\n",
    "`train_test_split` returns a tuple of our features/response split into training and test sets. The `random_state` argument shows up in a lot of places in `sklearn`.  Generally, when there is a stochastic component to the `sklearn` component you are using, `random_state` is there to allow you to set a seed so that your work can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - using the sample code in the previos cell, split the loss\n",
    "# reserving data into training and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Fitting our classifier\n",
    "Our data is in a numerical format, its been split, and now we are ready to do some Machine Learning.  \n",
    "\n",
    "Don't forget, when fitting any supervised learning technique, you must specify both your featureset and your response in the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Create a KNeighborsClassifier and fit it to our training data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Evaluating our classifier\n",
    "The `score()` method of all classifiers defaults to an accuracy measure.  For regressors, it will return an R-squared figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Use score() to evaluate the accuracy of our KNeighborsClassifier\n",
    "# on our test set.\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Classifier Confusion Matrix\n",
    "Another way of looking at a classifier's performance is by way of its `confusion_matrix` which gives a bit more information than our accuracy score.  Specifically, it tells us our false positive and false negative rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test, knn.predict(X_test)),\n",
    "             index=le.classes_, columns=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try another classifier\n",
    "Remember the `sklearn` API was designed to make using different algorithms as consistent as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - Fit a LogisticRegression and evaluate its accuracy on the test data.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression() # Swap in the another estimator\n",
    "model.fit(X_train, y_train) # No changes\n",
    "print(model.score(X_test,y_test)) # No changes\n",
    "pd.DataFrame(confusion_matrix(y_test, model.predict(X_test)),\n",
    "             index=le.classes_, columns=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visual representation of first three Development Factors\n",
    "By inspection (at least across the first three development ages), it is more difficult to distinguish between `wkcomp` and `ppauto` in line with where our classifiers are least accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "g = sns.pairplot(ldf.reset_index()[['LOB','1-2','2-3','3-4']], hue=\"LOB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Robustness of train_test_split\n",
    "Let's test the performance of our `KNeighborsClassifier` using a different random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=41)\n",
    "# Exercise - What is our accuracy when we change our train_test_split\n",
    "# random_state to 41?\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Cross-validation\n",
    "* `sklearn` provides a `cross_val_score` to test the accuracy of an estimator across multiple folds painting a truer picture of an estimators' efficacy than a simple train/test split.\n",
    "* With `cross_val_score`, we don't really need to provide separate train and test sets.  Though, with enough data, it is sometimes instructive to have train/test and holdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "np.mean(cross_val_score(knn, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Improving model accuracy with GridSearchCV\n",
    "\n",
    "With `GridSearchCV`, we can feed a hyperparameter grid into our estimator to determine an 'optimal' set of hyperparameters to use for our particular business problem.  `GridSearchCV` itself is an estimator and so it has the usual `'fit()` and `predict()` methods any other classifier would.\n",
    "\n",
    "At a minimum, parameterizing the GridSearchCV estimator we need to specify:\n",
    "1. The estimator we want to use\n",
    "2. The hyperparameter searchspace as a dictionary\n",
    "\n",
    "Optionally, we can also specify:\n",
    "1. The number of folds to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={'n_neighbors':[1,3,5,7,9,11]}\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, refit=True)\n",
    "grid.fit(X, y)\n",
    "print(f'Best Accuracy Score: {grid.best_score_}')\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Visual inspection of the cross-validated scores shows support for `n_neighbors=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pointplot(x=grid.cv_results_['param_n_neighbors'],\n",
    "                  y=grid.cv_results_['mean_test_score']) \\\n",
    "       .set(xlabel='n_neighbors', ylabel='Accuracy', title='Gridsearch Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Explore values of KNeighborsClassifier hyperparameter, p to\n",
    "# improve the cross-validated score of our estimator\n",
    "param_grid=dict(n_neighbors=[1,3,5,7,9,11], p=[1,2,3,4,5,6])\n",
    "grid = GridSearchCV(knn, param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "print(f'Best Score: {grid.best_score_}')\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Display the confusion matrix of your best estimator from the\n",
    "# previous exercise on the entire dataset\n",
    "pd.DataFrame(confusion_matrix(y, grid.best_estimator_.predict(X)),\n",
    "             index=le.classes_, columns=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More complex workflows with Pipeline\n",
    "\n",
    "The authors of `sklearn` recognize that composability of multiple estimators will be necessary to build the best models.  For example, you may want to cluster a feature before feeding it into a Regressor.\n",
    "\n",
    "The `Pipeline` is useful for chaining one or more transformers together.  Pipelines themselves are estimators and have `fit()`, `predict()`, and `score()` function and can be used with all of the `sklearn` funcitons used for regular estimators including but not limited to: `cross_val_score`, `confusion_martix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Regression Dataset\n",
    "boston = load_boston()\n",
    "    \n",
    "# Polynomial Transformer\n",
    "poly = PolynomialFeatures(degree=2) \n",
    "\n",
    "# Regressor\n",
    "rf = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "# Set up Steps\n",
    "steps=[('poly', poly), ('rf', rf)]\n",
    "\n",
    "# Build a Pipeline\n",
    "pipe = Pipeline(steps=steps) \n",
    "\n",
    "# Cross-Validation R-square\n",
    "np.mean(cross_val_score(pipe, boston['data'], boston['target'], cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Create a Pipeline with Principle Components Analysis (PCA) as a\n",
    "# first step and the optimal KNeighbors hyperparameters (n_neighbors=3 and p=3)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "steps=[('pca', PCA()),\n",
    "       ('knn',KNeighborsClassifier(n_neighbors=3, p=3))]\n",
    "\n",
    "pipe = Pipeline(steps=steps)\n",
    "np.mean(cross_val_score(pipe, X, y,cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pipelines and GridSearchCV\n",
    "Since a `Pipeline` is just another estimator `GridSearchCV` allows the hyperparameter space of all estimators in the pipeline to be gridsearched in one go.  \n",
    "\n",
    "To avoid hyperparameter name clashes between one estimator and another within a pipeline, `sklearn` uses a double underscore naming convention of the form {estimator_name}__{hyperparameter} for the keys of its parameter grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = dict(rf__n_estimators=[10, 25],\n",
    "                  rf__max_depth=[10, 15, 20],\n",
    "                  rf__min_samples_split=[5, 10, 15],\n",
    "                  poly__degree=[1, 2])\n",
    "\n",
    "# Set up Steps\n",
    "steps=[('poly', PolynomialFeatures()), ('rf', RandomForestRegressor())]\n",
    "\n",
    "# Build a Pipeline\n",
    "pipe = Pipeline(steps=steps) \n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = dict(rf__n_estimators=[10, 25],\n",
    "                  rf__max_depth=[10, 15, 20],\n",
    "                  rf__min_samples_split=[5, 10, 15],\n",
    "                  poly__degree=[1, 2])\n",
    "\n",
    "#Grid Search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, iid=True)\n",
    "grid.fit(boston['data'], boston['target'])\n",
    "\n",
    "print(f'Best R-square: {grid.best_score_}')\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise - Use Pipeline and GridsearchCV to determine whether we achieve a\n",
    "# better cross-validated accuracy using PCA and KNeighborsClassifier combined\n",
    "\n",
    "param_grid=dict(knn__n_neighbors=[1,3,5,7,9,11],\n",
    "                knn__p=[1,2,3,4,5,6],\n",
    "                pca__n_components=[3, 5, 7, 9])\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', PCA()),\n",
    "                       ('knn',KNeighborsClassifier())])\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, refit=True)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(f'Best Score: {grid.best_score_}')\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scikit-Learn Recap\n",
    "\n",
    "* Almost everything is an Estimator.  They all have a `fit` method and depending on the nature of the estimator may also have a `predict`, `score` or `transform` method.\n",
    "* The API is standardized across estimator\n",
    "* A transformer is a special type of estimator that trasnforms data for another Estimator\n",
    "* Cross-validation with Grid Search helps in hyperparameter selection\n",
    "* Pipelines are useful for composing a chain of Estimators.\n",
    "* The documentation is a goldmine of information"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
