{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas = **PAN**el **DA**ta**S**ets\n",
    "\n",
    "\n",
    ">  *pandas* provides high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "* General purpose data munger and ```numpy``` array wrapper  \n",
    "* Persist to / read from variety of data sources including Excel \n",
    "* Two core data structures: a ```Series``` for 1d data and a ```DataFrame``` for 2d data\n",
    "* ```DataFrames``` are indexed by rows and columns and all operations are index-aware\n",
    "* Joins/merge\n",
    "* Summarize, transform\n",
    "* melt, stack/unstack, pivot tables\n",
    "* Excellent time series support \n",
    "* Good integration with Jupyter for viewing data \n",
    "* Graceful handling of missing values \n",
    "* Nice integration with Python string handling \n",
    "* Plotting\n",
    "\n",
    "See [10 minute intro to pandas](http://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html).\n",
    "\n",
    "## Functions We Will Discuss\n",
    "\n",
    "* DataFrame\n",
    "* head, tail, describe\n",
    "* unique, value_counts\n",
    "* read_csv\n",
    "* loc, slices, xs\n",
    "* create_index, reset_index \n",
    "* MultiIndex \n",
    "* query \n",
    "* pivot, stack and unstack\n",
    "* **concat**, append, keys \n",
    "* pivot_table (crosstab), pivot \n",
    "* **merge** (indicator) and join\n",
    "* groupby (.groups, .get_group, as_index)\n",
    "* sum, mean, std etc. \n",
    "* aggregate\n",
    "* transform (same size as input whiten)\n",
    "* apply\n",
    "* assign \n",
    "* plot\n",
    "\n",
    "## Functions not covered but check out on your own\n",
    "* map (series), applymap (dataframes) \n",
    "* from_dict\n",
    "* rename \n",
    "* melt\n",
    "* evaluate \n",
    "* str\n",
    "* dt\n",
    "* style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn Plotting \n",
    "\n",
    "```pandas``` + ```seaborn``` $\\approx$ ```tibbles``` + ```ggplot```\n",
    "\n",
    "* **```relplot```**  = relational plots, line, scatter \n",
    "* catplot = scatter plot with categorical, box, swarm, bar, count \n",
    "* jointplot, pairplot, distplot, kdeplot\n",
    "* lmplot, regplot, residplot \n",
    "* heatmap, clustermap \n",
    "* faceting, row/column plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basics \n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "import re\n",
    "\n",
    "# nice printing of dataframes \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# other setup \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "# %load_ext line_profiler\n",
    "np.set_printoptions(linewidth =  160)\n",
    "\n",
    "# handy utility \n",
    "import textwrap \n",
    "def wdid(ob, ex=False):\n",
    "    ''' what does object do?, ex=True for more information \n",
    "    '''\n",
    "    print('\\n'.join(textwrap.wrap(' '.join([i for i in dir(ob) if i[0] != '_']), 80)))\n",
    "    if ex:\n",
    "    # optional pause for something more advanced... \n",
    "        for m in [ i for i in dir(np) if i[0] >= 'a' and i[0]<='z']:\n",
    "            print(f'\\n\\n{m}\\n{\"=\"*len(m)}\\n')\n",
    "            print(np.__getattribute__(m).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn==0.9.0\n",
    "assert sns.__version__ == '0.9.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why I love Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the emailed list of attendees\n",
    "attendees = '''Allard, Christopher [Westfield Insurance]\tchristopherallard@westfieldgrp.com\tForced Subscription\n",
    "Bogaardt, John [WCF Insurance]\tjbogaardt@gmail.com\tForced Subscription\n",
    "Chiarolanza, Laura [Chubb]\tlaura.chiarolanza@chubb.com\tForced Subscription\n",
    "Citarella, Christian [New Hampshire Insurance Department]\tchristian.citarella@ins.nh.gov\tForced Subscription\n",
    "Edwards, Vincent [Casualty Actuarial Society]\tvedwards@casact.org\tForced Subscription\n",
    "Faber, Brian [CNA Insurance Companies]\tbrian.a.faber@gmail.com\tForced Subscription\n",
    "Fannin, Brian [Casualty Actuarial Society]\tbfannin@casact.org\tForced Subscription\n",
    "Govonlu, David [Homesite Insurance]\tdgovonlu@homesite.com\tForced Subscription\n",
    "Granados, Marcela [EY]\tmagramore@gmail.com\tForced Subscription\n",
    "Greenberg, Eric [Rivington Partners]\tegreenberg@valeip.com\tForced Subscription\n",
    "Groeschen, Steven [Demotech, Inc.]\tsgroeschen@demotech.com\tForced Subscription\n",
    "Jablonski, Jeffrey [Erie Insurance Group]\tj2195@erieinsurance.com\tForced Subscription\n",
    "Kamykowski, Theresa [American Association of Insurance Services]\ttheresa.kamykowski@agcs.allianz.com\tForced Subscription\n",
    "Keim, Scott [Ameriprise Auto & Home Insurance]\tscott.keim@ampf.com\tForced Subscription\n",
    "Kelch, Michael [Pinnacle Actuarial Resources]\tmikekelch21@gmail.com\tForced Subscription\n",
    "Lee, Joyce [AXIS Capital]\tjoyce.lee@axiscapital.com\tForced Subscription\n",
    "Lo, Anson [TD Insurance]\tanson.lo@tdinsurance.com\tForced Subscription\n",
    "Mildenhall, Stephen [St. John's University]\tmildenhs@stjohns.edu\tForced Subscription\n",
    "Perry, Christopher [American Modern Insurance Group]\tcperry@amig.com\tForced Subscription\n",
    "Picard, Mathieu [Genworth Financial]\tmathieu.picard@genworth.com\tForced Subscription\n",
    "Qureshi, Abdul [AXIS Capital]\tabdul.qureshi@axiscapital.com\tForced Subscription\n",
    "Roddy, Matthew [Federated Insurance Companies]\tmrroddy@fedins.com\tForced Subscription\n",
    "Sobel, Scott [Oliver Wyman Actuarial Consulting]\tscott.sobel@oliverwyman.com\tForced Subscription\n",
    "Woodruff, Arlene [MLMIC Services, Inc.]\tawoodruff@mlmic.com\tForced Subscription\n",
    "Yskes, Eric [Amerisure Companies]'''\n",
    "\n",
    "adf = pd.DataFrame([[j.strip() for j in re.split('\\[|\\]|\\t', i) if j!=''][:-1] for i in attendees.split('\\n') ], \n",
    "                 columns = ['Name', 'Employer', 'Contact'])\n",
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do that again, only spread out\n",
    "att_split =  attendees.split('\\n')\n",
    "print('\\n'.join(att_split[:3]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plit each row at [, ] or tab\n",
    "row_split = re.split('\\[|\\]|\\t', att_split[0])\n",
    "print(row_split, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the empty string and drop last element\n",
    "row_split = [i for i in row_split[:-1] if len(i)>0]\n",
    "print(row_split, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into data frame---actually weirdly hard for a single row...  \n",
    "temp = pd.DataFrame(row_split) \n",
    "display(temp)\n",
    "pd.DataFrame({ i: j for i, j in zip(['Name', 'Employer', 'Contact'], row_split)}, index=[0])\n",
    "\n",
    "# iterate with list comprehensions and re-orient... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cake\n",
    "\n",
    "Read in an explore the CAS Loss Reserve Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'http://www.mynl.com/RPM/masterdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a few obvious columns: df['new col name'], refer to existing columns in one of threee ways, : = all \n",
    "df['LR'] = df.UltIncLoss / df['EarnedPrem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'PdLR'] = df.PaidLoss / df.loc[:, 'EarnedPrem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CaseLR'] = df.CaseIncLoss / df.iloc[:, 10]  # obviously a terrible method; df.columns.get_loc('EarnedPrem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some company names for future use\n",
    "sfm = 'State Farm Mut Grp'\n",
    "amg = 'American Modern Ins Grp Inc'\n",
    "eix = 'Erie Ins Exchange Grp'\n",
    "fmg = 'Federated Mut Grp'\n",
    "wbi = 'West Bend Mut Ins Grp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "* Pull out the ```PaidLoss``` column\n",
    "* What do you think ```df.loc[1600, :]``` means? Or ```df.loc[1600, 'EarnedPrem']```? Try them...in the spcae below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=df.query(' EarnedPrem > 50000'), kind='line', x='AY', y=\"LR\", hue='Lag', \n",
    "            col='Line', col_wrap=3, palette=sns.color_palette(\"coolwarm\", 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=df.query(' EarnedPrem > 50000'), kind='line', x='AY', y=\"CaseLR\", hue='Lag', \n",
    "            col='Line', col_wrap=3, palette=sns.color_palette(\"coolwarm\", 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=df.query(' EarnedPrem > 50000'), kind='line', x='AY', y=\"PdLR\", hue='Lag', \n",
    "            col='Line', col_wrap=3, palette=sns.color_palette(\"coolwarm\", 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of PP Auto Results\n",
    "a = sns.distplot(df.query(' Lag == 10 and LR>=0 and LR<=3 and Line==\"PP Auto\" ')['LR'], kde=False, \n",
    "                 hist_kws=dict(edgecolor=\"w\", linewidth=1))\n",
    "a.set(xlim=[0,3], ylabel='Density', title='Histogram of Loss Ratios');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.relplot(data=df.query(' Lag==10 and GRName == \"State Farm Mut Grp\" '), kind='line', x='AY', y=\"LR\", hue='Line');\n",
    "ax.set(title=f'{sfm} Loss Ratios by AY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "* Create histogram of paid loss ratio\n",
    "* ...for commercial auto (Comm Auto)\n",
    "* ...for companies with premium > 10000 (EarnedPrem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns with regular expression (string matching)\n",
    "df.filter(regex='LR').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop our loss ratio columns to tidy up \n",
    "df = df.drop(df.filter(regex='LR').columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull off a column in multiple ways \n",
    "df.head()['AY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()[['AY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().AY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().loc[:, ['AY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().loc['AY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull off row with index == 2\n",
    "df.loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with indexes 2..4 (inclusive)\n",
    "df.loc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Line.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Line.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.GRName.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.GRName.unique()[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in df.GRName.unique() if i.find('State') >=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access rows by row number \n",
    "df.iloc[2:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "* How is ```df.iloc[2:4]``` different to ```df.loc[2:4]```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access rows and columns by row/column number \n",
    "df.iloc[14:4:-2, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by value, note ==\n",
    "df[df.GRName==sfm].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query by value with SQL like syntax\n",
    "df.query( 'GRName == \"State Farm Mut Grp\" ').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to variables with @ inside query or f strings\n",
    "display(df.query( 'GRName == @sfm ').head())\n",
    "df.query( f'GRName == \"{sfm}\" ').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't mix loc and iloc subset columns \n",
    "df[['Line', 'GRName', 'AY', 'DY', 'UltIncLoss', 'PaidLoss', 'CaseIncLoss', 'BulkLoss', 'EarnedPrem']].iloc[100:105, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries and ```groupby```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby: workhorse grouping...e.g. aggregate\n",
    "df.query(' DY==1997 ').groupby('Line').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sensible columns\n",
    "df.query(' DY==1997 ').groupby('Line')[['UltIncLoss', 'PaidLoss', 'CaseIncLoss', 'BulkLoss', 'EarnedPrem', 'PostedReserve97']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average loss ratios\n",
    "line_ay = df.query(' DY==1997 ').groupby(['Line', 'AY'])[['UltIncLoss', 'PaidLoss', \n",
    "                                                          'CaseIncLoss', 'BulkLoss', 'EarnedPrem', 'PostedReserve97']].sum()\n",
    "display(line_ay.head())\n",
    "line_ay['LR'] = line_ay.UltIncLoss / line_ay.EarnedPrem\n",
    "display(line_ay.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "* Add columns to ```line_ay``` for paid and case incurred loss ratios\n",
    "* Extract just the loss ratio columns using ```[[column names]]```  to produce a report\n",
    "\n",
    "        Line        AY     UltLR   CaseLR   PaidLR\n",
    "        Comm Auto   1988   xxx     xxx      xxx\n",
    "        etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from index have stack and unstack, like tidyverse gather and spread\n",
    "line_ay[['LR']].unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_ay[['LR']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns and indexes are often interchangeable (they will be eventually) but not always...\n",
    "sns.relplot(data=line_ay[['LR']], x='AY', y=\"LR\", kind='line', hue='Line') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when indexes and columns are NOT interchangeable,  you need to convert\n",
    "line_ay[['LR']].reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=line_ay[['LR']].reset_index(), x='AY', y=\"LR\", kind='line', hue='Line');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ```groupby``` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the group by object? \n",
    "gb = df.query(' DY==1997 ').groupby(['Line', 'AY'])\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdid(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.get_group(('Comm Auto', 1990)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(' DY==1997 ').groupby(['Line', 'AY'])[['UltIncLoss', 'EarnedPrem']].apply(sum).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "g = df.query(' DY==1997 ').groupby(['Line', 'AY'])\n",
    "temp = g[['UltIncLoss']].sum()\n",
    "# display(temp.head(10))\n",
    "temp['EarnedPrem'] = g[['EarnedPrem']].sum()\n",
    "temp['LR'] = temp.UltIncLoss / temp.EarnedPrem\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# much slower...\n",
    "temp = df.query(' DY==1997 ').groupby(['Line', 'AY'])[['UltIncLoss', 'EarnedPrem']].apply(\n",
    "    lambda x : pd.Series([x.UltIncLoss.sum(), x.EarnedPrem.sum(), x.UltIncLoss.sum() / x.EarnedPrem.sum()],  index=['IL', 'EP', 'LR']))\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "temp = df.query(' DY==1997 ').groupby(['Line', 'AY'])[['UltIncLoss', 'EarnedPrem']].apply(\n",
    "    lambda x : pd.Series([x.UltIncLoss.sum(), x.EarnedPrem.sum()],  index=['IL', 'EP'])).assign(LR = lambda x : x.IL / x.EP)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when answer is one row per group can use agg[regate] shorthand, very flexible report building\n",
    "df.query(' DY==1997 ').groupby(['Line', 'AY']).agg({'UltIncLoss' : [np.max, np.min, np.mean], 'EarnedPrem': np.mean})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new data frame with more useful index\n",
    "df1 = df[['Line', 'GRName', 'AY', 'Lag', 'UltIncLoss', 'PaidLoss', 'CaseIncLoss', 'BulkLoss', 'EarnedPrem']]. \\\n",
    "    set_index(['GRName', 'Line', 'AY', 'Lag'])\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access rows by index = loc rather than integer-row number iloc \n",
    "df1.loc[sfm, ['UltIncLoss', 'EarnedPrem']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[[sfm]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You cannot mix ```.loc``` and ```.iloc```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're Actuaries: Let's Make Some Triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = df1.query('AY + Lag <= 1998')\n",
    "latest.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note query works seamlessly with indexes or columns \n",
    "df.query('AY + Lag <= 1998').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest.pivot_table(index=['Line', 'GRName', 'AY'] , columns='Lag', values='PaidLoss').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paid and incurred triangles\n",
    "triangles = latest.pivot_table(index=['GRName', 'Line', 'AY'] , columns='Lag', values=['PaidLoss', 'CaseIncLoss'])\n",
    "triangles.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles.loc[sfm, 'PaidLoss'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles.xs((sfm, 'Comm Auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make development factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by looking at one triangle\n",
    "trg = triangles.loc[sfm, 'PaidLoss'].head(10)\n",
    "trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link ratios should be devel 2nd:10th col / devel 1st:9th\n",
    "# don't have to worry about specifying the 10 and 9 and remember zero based arrays\n",
    "# 1: will be cols 2:10, :-1 cols 1:9\n",
    "trg.iloc[:, 1:] / trg.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes ususally helpful, but not always!\n",
    "# can work with values (underlying numpy array) but then lose index\n",
    "trg.iloc[:, 1:].values  / trg.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best of both worlds: pick up the index from cols 1:9 (the denominator)\n",
    "trg.iloc[:, 1:].values   / trg.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make **all** the development factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note ['PaidLoss'] to retain the second index level \n",
    "triangles.loc[:, 'PaidLoss'].iloc[:, 1:].values / triangles.loc[:, ['PaidLoss']].iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitch together\n",
    "\n",
    "Same approach used for the loss ratio report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat( list ) combines dataframes, axis=0 stacks vertially and axis=1 horizontally\n",
    "t2 = pd.concat((triangles, \n",
    "                triangles.loc[:, 'CaseIncLoss'].iloc[:, 1:].values / triangles.loc[:, ['CaseIncLoss']].iloc[:, :-1],\n",
    "                triangles.loc[:, 'PaidLoss'].iloc[:, 1:].values / triangles.loc[:, ['PaidLoss']].iloc[:, :-1]), axis=1)\n",
    "\n",
    "# need to make a suitable index...which is a tad tricky...go by hand\n",
    "t2.columns = pd.MultiIndex.from_tuples([(t, l) for t in ['CaseIncLoss', 'PaidLoss'] for l in range(1,11) ] + \n",
    "                         [(t, l) for t in ['CaseIncLink', 'PaidLink'] for l in range(1,10)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.loc[sfm, :].filter(regex='Paid').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.loc[sfm, :].filter(regex='Case').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get rid of the empty triangles...count the nas...should be 180 in complete triangles and more in incomplete ones \n",
    "t2.groupby(level=['GRName', 'Line']).apply(lambda x : x.isna().sum().sum()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out to just the complete triangles with exactly 180 missing entries\n",
    "complete = t2.groupby(level=['GRName', 'Line']).filter(lambda x : x.isna().sum().sum()==180)\n",
    "complete.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about **average** link ratios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskex(n, n_ays, kind, tiles):\n",
    "    \"\"\" \n",
    "    mask for avg last n years in a n_ays x n_ays triangle\n",
    "    \"\"\"\n",
    "    # size of link ratio triangle is one smaller than number of ays\n",
    "    nyrs = n_ays - 1\n",
    "    if kind=='loss_den':\n",
    "        ans = np.array([[1 if i + j < nyrs and i + j >= nyrs - n else 0 for i in range(n_ays)] for j in range(n_ays)])\n",
    "    elif kind=='loss_num':\n",
    "        ans = np.array([[1 if i > 0 and i + j < n_ays and i + j >= n_ays - n else 0 for i in range(n_ays)] for j in range(n_ays)])\n",
    "    else:\n",
    "        ans = np.array([[1 if i + j < nyrs and i + j >= nyrs - n else 0 for i in range(nyrs)] for j in range(n_ays)])\n",
    "    return np.tile(ans, (tiles, 2))\n",
    "\n",
    "def mask_count(n, size):\n",
    "    '''\n",
    "    size of mask for averaging link ratios \n",
    "    '''\n",
    "    n = min(n, size-1)\n",
    "    return np.tile(np.array([n]*(size-n-1) + list(range(n,0,-1))), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute three year average link ratio want these LDFs\n",
    "maskex(3, 10, 'link', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute staight and weighted average last 3, 5, all years, for paid and incurred loss = 2 x 2 x 3 = 12 methods:\n",
    "report = pd.concat([(complete.filter(regex='Loss', axis=1) * maskex(i, 10, 'loss_num', 400)).iloc[:, pd.np.r_[1:10, 11:20]].groupby(level=[0,1]).sum().values / \\\n",
    "           (complete.filter(regex='Loss', axis=1) * maskex(i, 10, 'loss_den', 400)).iloc[:, pd.np.r_[0:9, 10:19]].groupby(level=[0,1]).sum() for i in [3, 5, 10]]+\n",
    "           [(complete.filter(regex='Link', axis=1) * maskex(i, 10, 'link', 400)).groupby(level=[0,1]).sum() / mask_count(i, 10) for i in [3, 5, 10]],\n",
    "                    axis=1,\n",
    "                 keys=[(wt, i) for wt in ['Wtd', 'Str'] for i in [3, 5, 10]] ) \n",
    "report.columns.names= ['Method', 'NYrs', 'LossType', 'DY']\n",
    "report = report.stack(level=(0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.loc[sfm].head(12).sort_index(level=[0,1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find large companies within each line \n",
    "n_large = 20  # number of cos by line \n",
    "large_cos = df.query(' DY == 1998 ').groupby('Line').apply(\n",
    "    lambda x : x.groupby('GRName')[['EarnedPrem']].sum().sort_values(by='EarnedPrem', ascending=False).head(n_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of lines for each company \n",
    "large_cos.reset_index().GRName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ldfs for large companies: right merge report with large_cos\n",
    "# need to reshape report to have index GRName and Line \n",
    "report.reset_index(level=[2,3,4], drop=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just large cos with right merge; inner merge \n",
    "ldfs = report.reset_index(drop=False).merge(large_cos.reset_index(drop=False), \n",
    "                                            how='inner', on=['GRName', 'Line']).reset_index(drop=True)\n",
    "ldfs.LossType = ldfs.LossType.str[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy data-like melt function for general re-shaping \n",
    "p = pd.melt(ldfs, id_vars=('GRName', 'Line', \"Method\", 'NYrs', 'LossType', 'EarnedPrem'), value_name='ldf', var_name='Age'). \\\n",
    "    sort_values(['GRName', 'Line', \"Method\", 'NYrs', 'LossType', 'Age'])\n",
    "p['logldf'] = np.log(p.ldf-1)\n",
    "p.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.relplot(data=p, kind='line', x='Age', y='ldf', hue='LossType', col='Line', col_wrap=3);\n",
    "for ax in a.axes:\n",
    "    ax.set(ylim=[0.5,3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.relplot(data=p, kind='line', x='Age', y='logldf', style='NYrs', hue='LossType', col='Line', col_wrap=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.relplot(data=p, kind='line', x='Age', y='logldf', style='Method', hue='LossType', col='Line', col_wrap=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare paid and incurred methdods by company \n",
    "sns.set(style='whitegrid')\n",
    "a = sns.relplot(data=p, kind='line', x='Age', y='logldf', hue='GRName', row='Line', col='LossType', legend=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If there is time...let's do some simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_inc_plot(df, co_name='', line_name='', bins=201, dd=True, ax=None, legend=False):\n",
    "    '''\n",
    "    bootstrap from paid and incurred and create product distribution \n",
    "    input is result of running\n",
    "    \n",
    "        links = comp.groupby(level=['GRName', 'Line']).apply(make_links)\n",
    "        links.index.names = ['GRName', 'Line', 'Kind', 'Method']\n",
    "    \n",
    "    i.e. df has index GRName, Line, AY and col groups for Paid, CaseInc loss and links  and lag \n",
    "    '''\n",
    "\n",
    "    def shorten(s):\n",
    "        '''\n",
    "        name shortening function for labels \n",
    "        '''\n",
    "        if len(s) < 12:\n",
    "            return s\n",
    "        else:\n",
    "            re.sub\n",
    "            s = re.sub(' (Co|Ins|Grp|Exchange|Of|Inc|of)', '', s)\n",
    "            s = s.replace('Agricultural', 'Ag').replace('Exchange', 'Ex'). replace('Associated', 'Assoc')\n",
    "        if len(s) > 12:\n",
    "            s = ' '.join([i[:4] for i in s.split(' ')][:3])\n",
    "        return s\n",
    "    # allows use with groupby\n",
    "    if co_name == '':\n",
    "        co_name, line_name, _ = df.index[0]\n",
    "   \n",
    "    yrs = list(df.index.get_level_values('AY').unique())\n",
    "    nyrs = yrs[-1] - yrs[0]\n",
    "    \n",
    "    # piece of interest\n",
    "    bit = df.xs((co_name, line_name), level=('GRName', 'Line'))\n",
    "    \n",
    "    if len(bit) < 10:\n",
    "        return\n",
    "    \n",
    "    # make kronecker products for i (kpi) and paid (kpp)\n",
    "    # pull off most recent year losses \n",
    "    kpi = np.array(bit.loc[yrs[-1], ('CaseIncLoss', 1)])\n",
    "    kpp = np.array(bit.loc[yrs[-1], ('PaidLoss', 1)])\n",
    "    \n",
    "    # and complete with link ratios \n",
    "    for i in range(0, nyrs):\n",
    "        kpp = np.kron(kpp, bit.loc[yrs[0]:yrs[0]+i, ('PaidLink', nyrs - i)])\n",
    "        kpi = np.kron(kpi, bit.loc[yrs[0]:yrs[0]+i, ('CaseIncLink', nyrs - i)])\n",
    "\n",
    "    ult = pd.DataFrame( {'inc' : kpi, 'pd' : kpp})\n",
    "    # stats \n",
    "    d = ult.describe().iloc[1:, :]\n",
    "    if dd:\n",
    "        display(d)\n",
    "    \n",
    "    if ax is None:\n",
    "        f = plt.figure()\n",
    "        a = f.gca()\n",
    "    else:\n",
    "        a = next(ax)\n",
    "    \n",
    "    bp = np.linspace(d.loc['min', :].min(), d.loc['max', :].max(), bins)\n",
    "    mnn = d.loc['mean', :].min()\n",
    "    mnx = d.loc['mean', :].max()\n",
    "    sd = d.loc['std', : ].max()\n",
    "    bp = np.linspace(max(0, mnn - 4*sd), mnx + 4*sd, bins)\n",
    "    npd,  _, _ = a.hist(kpp, bins=bp, color='b', alpha=0.5, label='Paid')\n",
    "    ninc, _, _ = a.hist(kpi, bins=bp, color='r', alpha=0.5, label='Incurred')\n",
    "    bay = ninc*npd / sum(ninc*npd) * sum(npd)\n",
    "    xs = (bp[1:]+bp[0:-1])/2\n",
    "    a.plot(xs, bay, '-g', label='Posterior')\n",
    "    if legend:\n",
    "        a.legend(frameon=False)\n",
    "    a.set(title='{:}/{:}\\nMLE={:,.1f}, CV(I/Pd)={:.3f}/{:.3f}'.format(shorten(co_name), line_name, xs[bay.argmax()]/1e3, \n",
    "                                                                *(d.loc['std']/d.loc['mean']) ))\n",
    "    return ult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(df, line='', co='', threshold=250000):\n",
    "    '''\n",
    "    all lines for given co or all cos for given line \n",
    "    '''\n",
    "    if line=='' and co=='':\n",
    "        return \n",
    "    \n",
    "    if line != '':\n",
    "        bit = df.query(f' Line==\"{line}\" ')        \n",
    "        ncos = len(bit) / 10 \n",
    "        nr = int(ncos/6)\n",
    "        if nr < ncos/6: nr += 1\n",
    "        f, ax = plt.subplots(nr, 6, figsize=(18, 2.4*nr))\n",
    "        ax = iter(ax.flatten())\n",
    "        \n",
    "    elif co != '':\n",
    "        bit = df.query(f' GRName==\"{co}\" ')\n",
    "        f, ax = plt.subplots(2, 3, figsize=(12,6))\n",
    "        ax = iter(ax.flatten())\n",
    "    \n",
    "    g = bit.groupby(['GRName', 'Line'])\n",
    "\n",
    "    l = True\n",
    "    for k, v in g.groups.items():\n",
    "        grp = bit.loc[v]\n",
    "        if grp.CaseIncLoss.sum().sum() > threshold:\n",
    "            ult = pd_inc_plot(grp, dd=False, ax=ax, legend=l)\n",
    "            l = False\n",
    "        \n",
    "    # tidy up \n",
    "    for a in ax:\n",
    "        f.delaxes(a)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for co in [ wbi, sfm]:\n",
    "    plot_all(complete, co=co);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(complete, 'Comm Auto', 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for John's Session\n",
    "\n",
    "# Exercise: figure out what this is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CAS data\n",
    "data_url = 'https://www.casact.org/research/reserve_data'\n",
    "lobs = ['medmal','ppauto','wkcomp', 'othliab', 'comauto', 'prodliab']\n",
    "data = pd.DataFrame()\n",
    "data = []\n",
    "columns = ['GRCODE','GRNAME','AccidentYear','DevelopmentYear','DevelopmentLag'\n",
    "           ,'IncurLoss', 'CumPaidLoss','BulkLoss','EarnedPremDIR'\n",
    "           ,'EarnedPremCeded','EarnedPremNet', 'Single','PostedReserve97']\n",
    "for lob in lobs:\n",
    "    file_url = f'{data_url}/{lob}_pos.csv'\n",
    "    subset = pd.read_csv(file_url, names=columns, skiprows=1)\n",
    "    subset['LOB'] = lob\n",
    "    data.append(subset)\n",
    "data1 = pd.concat(data)\n",
    "data = data1.query(\" DevelopmentYear <= 1997 \").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triangles(data, nlarge=20):\n",
    "    '''\n",
    "    make ldf triangles from CAS data for largest companies\n",
    "    '''\n",
    "    aggregates2 = data.query(' DevelopmentYear ==  1997 ').groupby(['LOB','GRNAME'])['IncurLoss'].sum() \n",
    "    top_by_lob = aggregates2.groupby(level='LOB').apply(lambda x : x.nlargest(nlarge).reset_index(level=0, drop=True))\n",
    "    \n",
    "    data_alt2 = data.merge(top_by_lob.to_frame(), how='left', left_on=['LOB','GRNAME'], right_index=True)\n",
    "    data_alt2.loc[data_alt2.loc[:,'IncurLoss_y'].isna(), 'GRNAME'] = 'Other'\n",
    "    \n",
    "    # create triangles \n",
    "    triangles = pd.pivot_table(data_alt2, index=['GRNAME','LOB','AccidentYear'],\n",
    "                           columns='DevelopmentLag', values='CumPaidLoss')\n",
    "    \n",
    "    # Determine LDF Weights \n",
    "    w = pd.DataFrame(np.array([[1 if i+j<9 else 0 for i in range(9)] for j in range(10)]))\n",
    "    weight = np.tile(w, (int(triangles.shape[0]/10), 1))\n",
    "    columns = [f'{triangles.columns[num]}-{triangles.columns[num+1]}'\n",
    "               for num, item in enumerate(triangles.columns[:-1])]\n",
    "\n",
    "    # Volume-weighted numerator and demoninator mask for denom only; values on num because want index from num \n",
    "    ldf = (triangles.iloc[:,1:].groupby(level=['GRNAME','LOB']).sum().values / \\\n",
    "           (weight*triangles.iloc[:,:-1]).groupby(level=['GRNAME','LOB']).sum()).fillna(1.0) \n",
    "    \n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john = make_triangles(data, 20)\n",
    "john.to_csv('trg-for-john.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should be the same as our ```ldf``` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = ldfs.query(' Method==\"Wtd\" and NYrs==10 and LossType==\"Paid\" ').drop(['Method', 'NYrs', 'LossType', 'EarnedPrem'], axis=1).\\\n",
    "    set_index(['GRName', 'Line'])\n",
    "l1.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
